{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"NeuralNetwork_pyTorch.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"q5pjU85f0jl8","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import torch\n","from torch import optim\n","from torch import nn\n","from torch.autograd import Variable\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","from LookAhead import Lookahead"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXY9VEoz0jmB","colab_type":"code","outputId":"c73b0815-fb7d-4952-89e3-3ae8adc8e608","colab":{}},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H2dQTfFi0jmE","colab_type":"code","colab":{}},"source":["writer = SummaryWriter('tensorboard')\n","save_file = 'state.pt'\n","device = torch.device('cuda:0')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0GZ-QT-0jmG","colab_type":"code","colab":{}},"source":["net_sgd = resnet18().to(device)\n","net_sgd_la = resnet18().to(device)\n","net_adam = resnet18().to(device)\n","net_adam_la = resnet18().to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jF60UuE30jmI","colab_type":"code","colab":{}},"source":["criterion_sgd = nn.CrossEntropyLoss()\n","criterion_sgd_la = nn.CrossEntropyLoss()\n","criterion_adam = nn.CrossEntropyLoss()\n","criterion_adam_la = nn.CrossEntropyLoss()\n","opt_sgd = optim.SGD(net_sgd.parameters(), lr=0.001, momentum=0.9)\n","opt_sgd_la = Lookahead(optim.SGD(net_sgd_la.parameters(), lr=0.001, momentum=0.9))\n","opt_adam = optim.Adam(net_adam.parameters())\n","opt_adam_la = Lookahead(optim.Adam(net_adam_la.parameters()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zq3ZJhTB0jmK","colab_type":"code","outputId":"b18155ca-d61f-49a2-8667-2b5498534f97","colab":{}},"source":["for epoch in range(15):\n","    running_loss = np.zeros(4)\n","    for i, data in enumerate(trainloader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        inputs, labels = Variable(inputs), Variable(labels)\n","        \n","        opt_sgd.zero_grad()\n","        opt_sgd_la.zero_grad()\n","        opt_adam.zero_grad()\n","        opt_adam_la.zero_grad()\n","        \n","        out_sgd = net_sgd(inputs)\n","        out_sgd_la = net_sgd_la(inputs)\n","        out_adam = net_adam(inputs)\n","        out_adam_la = net_adam_la(inputs)\n","        \n","        loss_sgd = criterion_sgd(out_sgd, labels)\n","        loss_sgd_la = criterion_sgd_la(out_sgd_la, labels)\n","        loss_adam = criterion_adam(out_adam, labels)\n","        loss_adam_la = criterion_adam_la(out_adam_la, labels)\n","        \n","        loss_sgd.backward()\n","        opt_sgd.step()\n","        \n","        loss_sgd_la.backward()\n","        opt_sgd_la.step()\n","        \n","        loss_adam.backward()\n","        opt_adam.step()\n","        \n","        loss_adam_la.backward()\n","        opt_adam_la.step()\n","        \n","        running_loss[0] += loss_sgd.item()\n","        running_loss[1] += loss_sgd_la.item()\n","        running_loss[2] += loss_adam.item()\n","        running_loss[3] += loss_adam_la.item()\n","        \n","        if i % 2000 == 1999:\n","            print('[%d, %5d] loss: [%.3f, %.3f, %.3f, %.3f]' %\n","                  (epoch + 1, i + 1, running_loss[0] / 2000, running_loss[1] / 2000,\n","                  running_loss[2] / 2000, running_loss[3] / 2000))\n","            writer.add_scalars('Loss', {'SGD': running_loss[0] / 2000,\n","                                       'Lookahead(SGD)': running_loss[1] / 2000,\n","                                       'Adam': running_loss[2] / 2000,\n","                                       'Lookahead(Adam)': running_loss[3] / 2000})\n","            running_loss = np.zeros(4)\n","    torch.save({'epoch': epoch+1,\n","               'sgd_state':[net_sgd.state_dict(), opt_sgd.state_dict()],\n","               'sgd_la_state':[net_sgd_la.state_dict(), opt_sgd_la.state_dict()],\n","               'adam_state':[net_adam.state_dict(), opt_adam.state_dict()],\n","               'adam_la_state':[net_adam_la.state_dict(), opt_adam_la.state_dict()]},\n","              save_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,  2000] loss: [2.397, 2.339, 2.291, 2.238]\n","[1,  4000] loss: [2.098, 2.011, 2.075, 2.062]\n","[1,  6000] loss: [1.989, 1.918, 2.038, 2.055]\n","[1,  8000] loss: [1.853, 1.798, 1.939, 1.896]\n","[1, 10000] loss: [1.793, 1.746, 1.818, 1.811]\n","[1, 12000] loss: [1.708, 1.664, 1.736, 1.731]\n","[2,  2000] loss: [1.635, 1.608, 1.640, 1.648]\n","[2,  4000] loss: [1.600, 1.551, 1.638, 1.631]\n","[2,  6000] loss: [1.534, 1.485, 1.555, 1.539]\n","[2,  8000] loss: [1.499, 1.479, 1.531, 1.538]\n","[2, 10000] loss: [1.437, 1.411, 1.447, 1.467]\n","[2, 12000] loss: [1.434, 1.399, 1.459, 1.430]\n","[3,  2000] loss: [1.368, 1.337, 1.404, 1.388]\n","[3,  4000] loss: [1.336, 1.328, 1.404, 1.357]\n","[3,  6000] loss: [1.276, 1.258, 1.358, 1.315]\n","[3,  8000] loss: [1.247, 1.246, 1.312, 1.284]\n","[3, 10000] loss: [1.238, 1.233, 1.295, 1.253]\n","[3, 12000] loss: [1.219, 1.209, 1.285, 1.244]\n","[4,  2000] loss: [1.154, 1.131, 1.227, 1.161]\n","[4,  4000] loss: [1.132, 1.127, 1.237, 1.200]\n","[4,  6000] loss: [1.134, 1.120, 1.242, 1.161]\n","[4,  8000] loss: [1.128, 1.118, 1.218, 1.136]\n","[4, 10000] loss: [1.094, 1.091, 1.193, 1.124]\n","[4, 12000] loss: [1.101, 1.090, 1.181, 1.170]\n","[5,  2000] loss: [1.000, 0.999, 1.123, 1.077]\n","[5,  4000] loss: [1.011, 1.017, 1.152, 1.104]\n","[5,  6000] loss: [1.027, 1.004, 1.140, 1.074]\n","[5,  8000] loss: [1.015, 0.998, 1.110, 1.090]\n","[5, 10000] loss: [0.976, 0.971, 1.091, 1.093]\n","[5, 12000] loss: [0.984, 0.985, 1.143, 1.062]\n","[6,  2000] loss: [0.912, 0.897, 1.016, 1.001]\n","[6,  4000] loss: [0.931, 0.906, 1.046, 1.013]\n","[6,  6000] loss: [0.900, 0.908, 1.032, 1.034]\n","[6,  8000] loss: [0.919, 0.917, 1.032, 1.026]\n","[6, 10000] loss: [0.928, 0.894, 1.008, 0.988]\n","[6, 12000] loss: [0.891, 0.904, 1.050, 0.994]\n","[7,  2000] loss: [0.820, 0.805, 0.990, 0.960]\n","[7,  4000] loss: [0.872, 0.865, 1.041, 0.989]\n","[7,  6000] loss: [0.813, 0.818, 0.961, 0.956]\n","[7,  8000] loss: [0.822, 0.848, 1.009, 0.965]\n","[7, 10000] loss: [0.854, 0.812, 0.984, 0.933]\n","[7, 12000] loss: [0.826, 0.812, 0.991, 0.938]\n","[8,  2000] loss: [0.754, 0.750, 0.959, 0.922]\n","[8,  4000] loss: [0.762, 0.749, 0.914, 0.927]\n","[8,  6000] loss: [0.767, 0.751, 0.921, 0.905]\n","[8,  8000] loss: [0.773, 0.762, 0.932, 0.911]\n","[8, 10000] loss: [0.759, 0.767, 0.934, 0.898]\n","[8, 12000] loss: [0.783, 0.757, 0.942, 0.900]\n","[9,  2000] loss: [0.698, 0.689, 0.893, 0.867]\n","[9,  4000] loss: [0.676, 0.672, 0.867, 0.843]\n","[9,  6000] loss: [0.698, 0.693, 0.888, 0.879]\n","[9,  8000] loss: [0.707, 0.837, 0.883, 0.964]\n","[9, 10000] loss: [0.702, 0.820, 0.893, 0.884]\n","[9, 12000] loss: [0.684, 0.739, 0.860, 0.871]\n","[10,  2000] loss: [0.616, 0.631, 0.818, 0.898]\n","[10,  4000] loss: [0.656, 0.686, 0.841, 0.886]\n","[10,  6000] loss: [0.633, 0.654, 0.868, 0.882]\n","[10,  8000] loss: [0.644, 0.656, 0.879, 0.835]\n","[10, 10000] loss: [0.639, 0.653, 0.835, 0.831]\n","[10, 12000] loss: [0.642, 0.667, 0.834, 0.816]\n","[11,  2000] loss: [0.537, 0.558, 0.794, 0.772]\n","[11,  4000] loss: [0.572, 0.592, 0.794, 0.812]\n","[11,  6000] loss: [0.598, 0.596, 0.843, 0.800]\n","[11,  8000] loss: [0.581, 0.572, 0.787, 0.767]\n","[11, 10000] loss: [0.874, 0.604, 0.817, 0.800]\n","[11, 12000] loss: [0.732, 0.609, 0.823, 0.821]\n","[12,  2000] loss: [0.584, 0.495, 0.769, 0.720]\n","[12,  4000] loss: [0.586, 0.533, 0.752, 0.733]\n","[12,  6000] loss: [0.585, 0.547, 0.757, 0.754]\n","[12,  8000] loss: [0.573, 0.564, 0.746, 0.757]\n","[12, 10000] loss: [0.584, 0.559, 0.812, 0.807]\n","[12, 12000] loss: [0.579, 0.552, 0.764, 0.754]\n","[13,  2000] loss: [0.476, 0.463, 0.716, 0.728]\n","[13,  4000] loss: [0.492, 0.470, 0.698, 0.709]\n","[13,  6000] loss: [0.516, 0.510, 0.763, 0.737]\n","[13,  8000] loss: [0.502, 0.496, 0.709, 0.764]\n","[13, 10000] loss: [0.517, 0.500, 0.726, 0.733]\n","[13, 12000] loss: [0.508, 0.506, 0.726, 0.735]\n","[14,  2000] loss: [0.412, 0.415, 0.676, 0.683]\n","[14,  4000] loss: [0.433, 0.460, 0.671, 0.692]\n","[14,  6000] loss: [0.449, 0.440, 0.676, 0.686]\n","[14,  8000] loss: [0.458, 0.448, 0.679, 0.691]\n","[14, 10000] loss: [0.475, 0.468, 0.694, 0.708]\n","[14, 12000] loss: [0.467, 0.473, 0.727, 0.717]\n","[15,  2000] loss: [0.373, 0.369, 0.613, 0.630]\n","[15,  4000] loss: [0.388, 0.399, 0.645, 0.677]\n","[15,  6000] loss: [0.396, 0.400, 0.671, 0.673]\n","[15,  8000] loss: [0.430, 0.425, 0.667, 0.652]\n","[15, 10000] loss: [0.418, 0.405, 0.670, 0.633]\n","[15, 12000] loss: [0.425, 0.418, 0.644, 0.697]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HGvXieNP0jmM","colab_type":"code","outputId":"71358bba-a940-415d-9cd7-df43a671b493","colab":{}},"source":["for epoch in range(15, 30):\n","    running_loss = np.zeros(4)\n","    for i, data in enumerate(trainloader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        inputs, labels = Variable(inputs), Variable(labels)\n","        \n","        opt_sgd.zero_grad()\n","        opt_sgd_la.zero_grad()\n","        opt_adam.zero_grad()\n","        opt_adam_la.zero_grad()\n","        \n","        out_sgd = net_sgd(inputs)\n","        out_sgd_la = net_sgd_la(inputs)\n","        out_adam = net_adam(inputs)\n","        out_adam_la = net_adam_la(inputs)\n","        \n","        loss_sgd = criterion_sgd(out_sgd, labels)\n","        loss_sgd_la = criterion_sgd_la(out_sgd_la, labels)\n","        loss_adam = criterion_adam(out_adam, labels)\n","        loss_adam_la = criterion_adam_la(out_adam_la, labels)\n","        \n","        loss_sgd.backward()\n","        opt_sgd.step()\n","        \n","        loss_sgd_la.backward()\n","        opt_sgd_la.step()\n","        \n","        loss_adam.backward()\n","        opt_adam.step()\n","        \n","        loss_adam_la.backward()\n","        opt_adam_la.step()\n","        \n","        running_loss[0] += loss_sgd.item()\n","        running_loss[1] += loss_sgd_la.item()\n","        running_loss[2] += loss_adam.item()\n","        running_loss[3] += loss_adam_la.item()\n","        \n","        if i % 2000 == 1999:\n","            print('[%d, %5d] loss: [%.3f, %.3f, %.3f, %.3f]' %\n","                  (epoch + 1, i + 1, running_loss[0] / 2000, running_loss[1] / 2000,\n","                  running_loss[2] / 2000, running_loss[3] / 2000))\n","            writer.add_scalars('Loss', {'SGD': running_loss[0] / 2000,\n","                                       'Lookahead(SGD)': running_loss[1] / 2000,\n","                                       'Adam': running_loss[2] / 2000,\n","                                       'Lookahead(Adam)': running_loss[3] / 2000})\n","            running_loss = np.zeros(4)\n","    torch.save({'epoch': epoch+1,\n","               'sgd_state':[net_sgd.state_dict(), opt_sgd.state_dict()],\n","               'sgd_la_state':[net_sgd_la.state_dict(), opt_sgd_la.state_dict()],\n","               'adam_state':[net_adam.state_dict(), opt_adam.state_dict()],\n","               'adam_la_state':[net_adam_la.state_dict(), opt_adam_la.state_dict()]},\n","              save_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[16,  2000] loss: [0.344, 0.333, 0.595, 0.642]\n","[16,  4000] loss: [0.362, 0.368, 0.607, 0.630]\n","[16,  6000] loss: [0.356, 0.365, 0.608, 0.644]\n","[16,  8000] loss: [0.371, 0.372, 0.614, 0.618]\n","[16, 10000] loss: [0.436, 0.396, 0.636, 0.626]\n","[16, 12000] loss: [0.403, 0.394, 0.613, 0.643]\n","[17,  2000] loss: [0.313, 0.310, 0.582, 0.573]\n","[17,  4000] loss: [0.316, 0.311, 0.581, 0.577]\n","[17,  6000] loss: [0.336, 0.319, 0.575, 0.597]\n","[17,  8000] loss: [0.352, 0.344, 0.597, 0.586]\n","[17, 10000] loss: [0.351, 0.338, 0.596, 0.610]\n","[17, 12000] loss: [0.379, 0.357, 0.588, 0.592]\n","[18,  2000] loss: [0.281, 0.266, 0.549, 0.574]\n","[18,  4000] loss: [0.286, 0.279, 0.525, 0.549]\n","[18,  6000] loss: [0.298, 0.279, 0.529, 0.545]\n","[18,  8000] loss: [0.305, 0.306, 0.534, 0.566]\n","[18, 10000] loss: [0.310, 0.310, 0.557, 0.567]\n","[18, 12000] loss: [0.330, 0.335, 0.598, 0.580]\n","[19,  2000] loss: [0.238, 0.236, 0.538, 0.499]\n","[19,  4000] loss: [0.246, 0.245, 0.496, 0.501]\n","[19,  6000] loss: [0.256, 0.271, 0.523, 0.510]\n","[19,  8000] loss: [0.292, 0.294, 0.528, 0.507]\n","[19, 10000] loss: [0.293, 0.310, 0.540, 0.530]\n","[19, 12000] loss: [0.316, 0.293, 0.562, 0.551]\n","[20,  2000] loss: [0.245, 0.217, 0.496, 0.461]\n","[20,  4000] loss: [0.245, 0.228, 0.547, 0.471]\n","[20,  6000] loss: [0.257, 0.247, 0.513, 0.500]\n","[20,  8000] loss: [0.268, 0.261, 0.532, 0.509]\n","[20, 10000] loss: [0.273, 0.267, 0.509, 0.494]\n","[20, 12000] loss: [0.271, 0.270, 0.532, 0.511]\n","[21,  2000] loss: [0.183, 0.183, 0.487, 0.459]\n","[21,  4000] loss: [0.209, 0.198, 0.477, 0.448]\n","[21,  6000] loss: [0.231, 0.227, 0.476, 0.445]\n","[21,  8000] loss: [0.227, 0.223, 0.512, 0.441]\n","[21, 10000] loss: [0.234, 0.234, 0.504, 0.467]\n","[21, 12000] loss: [0.240, 0.242, 0.521, 0.493]\n","[22,  2000] loss: [0.175, 0.176, 0.447, 0.395]\n","[22,  4000] loss: [0.184, 0.186, 0.503, 0.507]\n","[22,  6000] loss: [0.200, 0.197, 0.459, 0.445]\n","[22,  8000] loss: [0.204, 0.213, 0.504, 0.454]\n","[22, 10000] loss: [0.224, 0.227, 0.488, 0.474]\n","[22, 12000] loss: [0.229, 0.215, 0.570, 0.442]\n","[23,  2000] loss: [0.168, 0.161, 0.416, 0.406]\n","[23,  4000] loss: [0.163, 0.167, 0.434, 0.405]\n","[23,  6000] loss: [0.186, 0.172, 0.439, 0.411]\n","[23,  8000] loss: [0.210, 0.186, 0.453, 0.452]\n","[23, 10000] loss: [0.217, 0.202, 0.516, 0.454]\n","[23, 12000] loss: [0.204, 0.195, 0.455, 0.434]\n","[24,  2000] loss: [0.136, 0.141, 0.410, 0.428]\n","[24,  4000] loss: [0.144, 0.166, 0.421, 0.408]\n","[24,  6000] loss: [0.176, 0.165, 0.451, 0.439]\n","[24,  8000] loss: [0.173, 0.177, 0.506, 0.422]\n","[24, 10000] loss: [0.189, 0.185, 0.499, 0.425]\n","[24, 12000] loss: [0.179, 0.181, 0.457, 0.446]\n","[25,  2000] loss: [0.122, 0.144, 0.434, 0.373]\n","[25,  4000] loss: [0.122, 0.140, 0.409, 0.377]\n","[25,  6000] loss: [0.147, 0.139, 0.431, 0.418]\n","[25,  8000] loss: [0.151, 0.159, 0.442, 0.413]\n","[25, 10000] loss: [0.171, 0.173, 0.468, 0.401]\n","[25, 12000] loss: [0.199, 0.171, 0.442, 0.377]\n","[26,  2000] loss: [0.133, 0.122, 0.374, 0.369]\n","[26,  4000] loss: [0.135, 0.131, 0.409, 0.361]\n","[26,  6000] loss: [0.141, 0.132, 0.403, 0.412]\n","[26,  8000] loss: [0.147, 0.152, 0.449, 0.389]\n","[26, 10000] loss: [0.154, 0.166, 0.408, 0.385]\n","[26, 12000] loss: [0.158, 0.157, 0.421, 0.379]\n","[27,  2000] loss: [0.109, 0.105, 0.346, 0.327]\n","[27,  4000] loss: [0.117, 0.113, 0.389, 0.335]\n","[27,  6000] loss: [0.129, 0.121, 0.376, 0.350]\n","[27,  8000] loss: [0.127, 0.130, 0.378, 0.361]\n","[27, 10000] loss: [0.134, 0.132, 0.377, 0.378]\n","[27, 12000] loss: [0.153, 0.142, 0.418, 0.370]\n","[28,  2000] loss: [0.101, 0.103, 0.344, 0.324]\n","[28,  4000] loss: [0.104, 0.108, 0.357, 0.330]\n","[28,  6000] loss: [0.112, 0.115, 0.367, 0.350]\n","[28,  8000] loss: [0.121, 0.119, 0.379, 0.341]\n","[28, 10000] loss: [0.132, 0.114, 0.379, 0.355]\n","[28, 12000] loss: [0.130, 0.131, 0.357, 0.343]\n","[29,  2000] loss: [0.083, 0.094, 0.292, 0.285]\n","[29,  4000] loss: [0.095, 0.101, 0.351, 0.295]\n","[29,  6000] loss: [0.120, 0.102, 0.369, 0.335]\n","[29,  8000] loss: [0.113, 0.104, 0.372, 0.336]\n","[29, 10000] loss: [0.117, 0.113, 0.369, 0.328]\n","[29, 12000] loss: [0.123, 0.128, 0.413, 0.356]\n","[30,  2000] loss: [0.079, 0.083, 0.299, 0.317]\n","[30,  4000] loss: [0.087, 0.094, 0.330, 0.278]\n","[30,  6000] loss: [0.101, 0.100, 0.339, 0.319]\n","[30,  8000] loss: [0.123, 0.103, 0.380, 0.311]\n","[30, 10000] loss: [0.126, 0.114, 0.351, 0.337]\n","[30, 12000] loss: [0.107, 0.105, 0.375, 0.340]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jANNIbJg0jmO","colab_type":"code","outputId":"981b0255-6c60-45e6-ab0d-3070e86bd268","colab":{}},"source":["for epoch in range(30, 40):\n","    running_loss = np.zeros(4)\n","    for i, data in enumerate(trainloader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        inputs, labels = Variable(inputs), Variable(labels)\n","        \n","        opt_sgd.zero_grad()\n","        opt_sgd_la.zero_grad()\n","        opt_adam.zero_grad()\n","        opt_adam_la.zero_grad()\n","        \n","        out_sgd = net_sgd(inputs)\n","        out_sgd_la = net_sgd_la(inputs)\n","        out_adam = net_adam(inputs)\n","        out_adam_la = net_adam_la(inputs)\n","        \n","        loss_sgd = criterion_sgd(out_sgd, labels)\n","        loss_sgd_la = criterion_sgd_la(out_sgd_la, labels)\n","        loss_adam = criterion_adam(out_adam, labels)\n","        loss_adam_la = criterion_adam_la(out_adam_la, labels)\n","        \n","        loss_sgd.backward()\n","        opt_sgd.step()\n","        \n","        loss_sgd_la.backward()\n","        opt_sgd_la.step()\n","        \n","        loss_adam.backward()\n","        opt_adam.step()\n","        \n","        loss_adam_la.backward()\n","        opt_adam_la.step()\n","        \n","        running_loss[0] += loss_sgd.item()\n","        running_loss[1] += loss_sgd_la.item()\n","        running_loss[2] += loss_adam.item()\n","        running_loss[3] += loss_adam_la.item()\n","        \n","        if i % 2000 == 1999:\n","            print('[%d, %5d] loss: [%.3f, %.3f, %.3f, %.3f]' %\n","                  (epoch + 1, i + 1, running_loss[0] / 2000, running_loss[1] / 2000,\n","                  running_loss[2] / 2000, running_loss[3] / 2000))\n","            writer.add_scalars('Loss', {'SGD': running_loss[0] / 2000,\n","                                       'Lookahead(SGD)': running_loss[1] / 2000,\n","                                       'Adam': running_loss[2] / 2000,\n","                                       'Lookahead(Adam)': running_loss[3] / 2000})\n","            running_loss = np.zeros(4)\n","    torch.save({'epoch': epoch+1,\n","               'sgd_state':[net_sgd.state_dict(), opt_sgd.state_dict()],\n","               'sgd_la_state':[net_sgd_la.state_dict(), opt_sgd_la.state_dict()],\n","               'adam_state':[net_adam.state_dict(), opt_adam.state_dict()],\n","               'adam_la_state':[net_adam_la.state_dict(), opt_adam_la.state_dict()]},\n","              save_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[31,  2000] loss: [0.077, 0.120, 0.308, 0.277]\n","[31,  4000] loss: [0.087, 0.109, 0.327, 0.323]\n","[31,  6000] loss: [0.099, 0.104, 0.318, 0.295]\n","[31,  8000] loss: [0.100, 0.106, 0.336, 0.310]\n","[31, 10000] loss: [0.112, 0.121, 0.346, 0.316]\n","[31, 12000] loss: [0.108, 0.120, 0.348, 0.331]\n","[32,  2000] loss: [0.076, 0.069, 0.306, 0.270]\n","[32,  4000] loss: [0.082, 0.088, 0.280, 0.286]\n","[32,  6000] loss: [0.083, 0.087, 0.304, 0.321]\n","[32,  8000] loss: [0.080, 0.113, 0.311, 0.285]\n","[32, 10000] loss: [0.096, 0.105, 0.302, 0.296]\n","[32, 12000] loss: [0.079, 0.111, 0.309, 0.313]\n","[33,  2000] loss: [0.064, 0.066, 0.250, 0.242]\n","[33,  4000] loss: [0.066, 0.076, 0.272, 0.267]\n","[33,  6000] loss: [0.073, 0.077, 0.302, 0.265]\n","[33,  8000] loss: [0.080, 0.096, 0.289, 0.276]\n","[33, 10000] loss: [0.098, 0.083, 0.287, 0.282]\n","[33, 12000] loss: [0.096, 0.098, 0.292, 0.288]\n","[34,  2000] loss: [0.069, 0.062, 0.244, 0.224]\n","[34,  4000] loss: [0.072, 0.064, 0.283, 0.250]\n","[34,  6000] loss: [0.070, 0.082, 0.254, 0.254]\n","[34,  8000] loss: [0.079, 0.076, 0.291, 0.273]\n","[34, 10000] loss: [0.066, 0.084, 0.282, 0.297]\n","[34, 12000] loss: [0.105, 0.094, 0.274, 0.276]\n","[35,  2000] loss: [0.077, 0.059, 0.253, 0.237]\n","[35,  4000] loss: [0.078, 0.071, 0.266, 0.233]\n","[35,  6000] loss: [0.078, 0.064, 0.259, 0.235]\n","[35,  8000] loss: [0.070, 0.082, 0.257, 0.245]\n","[35, 10000] loss: [0.076, 0.071, 0.257, 0.240]\n","[35, 12000] loss: [0.078, 0.085, 0.278, 0.251]\n","[36,  2000] loss: [0.075, 0.055, 0.237, 0.222]\n","[36,  4000] loss: [0.072, 0.064, 0.225, 0.214]\n","[36,  6000] loss: [0.073, 0.052, 0.231, 0.217]\n","[36,  8000] loss: [0.084, 0.074, 0.250, 0.235]\n","[36, 10000] loss: [0.074, 0.068, 0.249, 0.240]\n","[36, 12000] loss: [0.070, 0.076, 0.249, 0.238]\n","[37,  2000] loss: [0.064, 0.057, 0.223, 0.193]\n","[37,  4000] loss: [0.048, 0.052, 0.225, 0.189]\n","[37,  6000] loss: [0.057, 0.055, 0.233, 0.221]\n","[37,  8000] loss: [0.070, 0.067, 0.248, 0.226]\n","[37, 10000] loss: [0.056, 0.059, 0.246, 0.226]\n","[37, 12000] loss: [0.080, 0.082, 0.273, 0.215]\n","[38,  2000] loss: [0.059, 0.058, 0.217, 0.223]\n","[38,  4000] loss: [0.058, 0.053, 0.207, 0.199]\n","[38,  6000] loss: [0.065, 0.053, 0.205, 0.208]\n","[38,  8000] loss: [0.076, 0.070, 0.238, 0.213]\n","[38, 10000] loss: [0.069, 0.072, 0.231, 0.227]\n","[38, 12000] loss: [0.065, 0.065, 0.258, 0.218]\n","[39,  2000] loss: [0.048, 0.059, 0.197, 0.168]\n","[39,  4000] loss: [0.056, 0.053, 0.219, 0.204]\n","[39,  6000] loss: [0.053, 0.049, 0.241, 0.226]\n","[39,  8000] loss: [0.056, 0.059, 0.199, 0.215]\n","[39, 10000] loss: [0.058, 0.063, 0.258, 0.198]\n","[39, 12000] loss: [0.057, 0.070, 0.238, 0.240]\n","[40,  2000] loss: [0.047, 0.053, 0.196, 0.206]\n","[40,  4000] loss: [0.058, 0.051, 0.207, 0.196]\n","[40,  6000] loss: [0.052, 0.049, 0.206, 0.192]\n","[40,  8000] loss: [0.062, 0.048, 0.216, 0.230]\n","[40, 10000] loss: [0.047, 0.052, 0.216, 0.206]\n","[40, 12000] loss: [0.052, 0.056, 0.223, 0.243]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QsolRQVt0jmQ","colab_type":"code","outputId":"4fc39963-b554-4076-9c8e-432a2fb27f9c","colab":{}},"source":["for epoch in range(40, 50):\n","    running_loss = np.zeros(4)\n","    for i, data in enumerate(trainloader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        inputs, labels = Variable(inputs), Variable(labels)\n","        \n","        opt_sgd.zero_grad()\n","        opt_sgd_la.zero_grad()\n","        opt_adam.zero_grad()\n","        opt_adam_la.zero_grad()\n","        \n","        out_sgd = net_sgd(inputs)\n","        out_sgd_la = net_sgd_la(inputs)\n","        out_adam = net_adam(inputs)\n","        out_adam_la = net_adam_la(inputs)\n","        \n","        loss_sgd = criterion_sgd(out_sgd, labels)\n","        loss_sgd_la = criterion_sgd_la(out_sgd_la, labels)\n","        loss_adam = criterion_adam(out_adam, labels)\n","        loss_adam_la = criterion_adam_la(out_adam_la, labels)\n","        \n","        loss_sgd.backward()\n","        opt_sgd.step()\n","        \n","        loss_sgd_la.backward()\n","        opt_sgd_la.step()\n","        \n","        loss_adam.backward()\n","        opt_adam.step()\n","        \n","        loss_adam_la.backward()\n","        opt_adam_la.step()\n","        \n","        running_loss[0] += loss_sgd.item()\n","        running_loss[1] += loss_sgd_la.item()\n","        running_loss[2] += loss_adam.item()\n","        running_loss[3] += loss_adam_la.item()\n","        \n","        if i % 2000 == 1999:\n","            print('[%d, %5d] loss: [%.3f, %.3f, %.3f, %.3f]' %\n","                  (epoch + 1, i + 1, running_loss[0] / 2000, running_loss[1] / 2000,\n","                  running_loss[2] / 2000, running_loss[3] / 2000))\n","            writer.add_scalars('Loss', {'SGD': running_loss[0] / 2000,\n","                                       'Lookahead(SGD)': running_loss[1] / 2000,\n","                                       'Adam': running_loss[2] / 2000,\n","                                       'Lookahead(Adam)': running_loss[3] / 2000})\n","            running_loss = np.zeros(4)\n","    torch.save({'epoch': epoch+1,\n","               'sgd_state':[net_sgd.state_dict(), opt_sgd.state_dict()],\n","               'sgd_la_state':[net_sgd_la.state_dict(), opt_sgd_la.state_dict()],\n","               'adam_state':[net_adam.state_dict(), opt_adam.state_dict()],\n","               'adam_la_state':[net_adam_la.state_dict(), opt_adam_la.state_dict()]},\n","              save_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[41,  2000] loss: [0.047, 0.036, 0.194, 0.166]\n","[41,  4000] loss: [0.054, 0.051, 0.204, 0.160]\n","[41,  6000] loss: [0.059, 0.050, 0.209, 0.221]\n","[41,  8000] loss: [0.061, 0.050, 0.194, 0.263]\n","[41, 10000] loss: [0.063, 0.057, 0.204, 0.235]\n","[41, 12000] loss: [0.079, 0.063, 0.215, 0.210]\n","[42,  2000] loss: [0.059, 0.048, 0.171, 0.173]\n","[42,  4000] loss: [0.044, 0.047, 0.181, 0.177]\n","[42,  6000] loss: [0.052, 0.046, 0.183, 0.193]\n","[42,  8000] loss: [0.052, 0.060, 0.208, 0.196]\n","[42, 10000] loss: [0.053, 0.060, 0.192, 0.197]\n","[42, 12000] loss: [0.058, 0.051, 0.188, 0.185]\n","[43,  2000] loss: [0.039, 0.038, 0.172, 0.151]\n","[43,  4000] loss: [0.038, 0.038, 0.172, 0.171]\n","[43,  6000] loss: [0.045, 0.050, 0.207, 0.180]\n","[43,  8000] loss: [0.056, 0.048, 0.197, 0.225]\n","[43, 10000] loss: [0.058, 0.042, 0.188, 0.209]\n","[43, 12000] loss: [0.053, 0.055, 0.195, 0.200]\n","[44,  2000] loss: [0.041, 0.045, 0.154, 0.161]\n","[44,  4000] loss: [0.043, 0.041, 0.160, 0.198]\n","[44,  6000] loss: [0.032, 0.047, 0.165, 0.193]\n","[44,  8000] loss: [0.044, 0.040, 0.164, 0.171]\n","[44, 10000] loss: [0.044, 0.063, 0.194, 0.210]\n","[44, 12000] loss: [0.041, 0.059, 0.213, 0.181]\n","[45,  2000] loss: [0.042, 0.030, 0.160, 0.157]\n","[45,  4000] loss: [0.054, 0.044, 0.190, 0.166]\n","[45,  6000] loss: [0.041, 0.052, 0.167, 0.164]\n","[45,  8000] loss: [0.045, 0.045, 0.215, 0.174]\n","[45, 10000] loss: [0.045, 0.045, 0.188, 0.162]\n","[45, 12000] loss: [0.050, 0.043, 0.198, 0.178]\n","[46,  2000] loss: [0.031, 0.038, 0.157, 0.163]\n","[46,  4000] loss: [0.038, 0.033, 0.190, 0.158]\n","[46,  6000] loss: [0.034, 0.036, 0.169, 0.168]\n","[46,  8000] loss: [0.041, 0.045, 0.188, 0.180]\n","[46, 10000] loss: [0.036, 0.054, 0.206, 0.161]\n","[46, 12000] loss: [0.038, 0.044, 0.215, 0.166]\n","[47,  2000] loss: [0.038, 0.029, 0.179, 0.147]\n","[47,  4000] loss: [0.030, 0.029, 0.167, 0.141]\n","[47,  6000] loss: [0.032, 0.044, 0.185, 0.159]\n","[47,  8000] loss: [0.038, 0.033, 0.172, 0.155]\n","[47, 10000] loss: [0.035, 0.049, 0.206, 0.168]\n","[47, 12000] loss: [0.041, 0.042, 0.185, 0.171]\n","[48,  2000] loss: [0.025, 0.031, 0.137, 0.141]\n","[48,  4000] loss: [0.029, 0.029, 0.149, 0.155]\n","[48,  6000] loss: [0.027, 0.037, 0.165, 0.162]\n","[48,  8000] loss: [0.027, 0.041, 0.188, 0.209]\n","[48, 10000] loss: [0.042, 0.041, 0.180, 0.151]\n","[48, 12000] loss: [0.033, 0.046, 0.193, 0.187]\n","[49,  2000] loss: [0.039, 0.042, 0.136, 0.138]\n","[49,  4000] loss: [0.029, 0.034, 0.141, 0.127]\n","[49,  6000] loss: [0.033, 0.038, 0.153, 0.162]\n","[49,  8000] loss: [0.037, 0.041, 0.165, 0.157]\n","[49, 10000] loss: [0.049, 0.065, 0.182, 0.151]\n","[49, 12000] loss: [0.051, 0.043, 0.162, 0.174]\n","[50,  2000] loss: [0.036, 0.042, 0.129, 0.146]\n","[50,  4000] loss: [0.031, 0.036, 0.147, 0.113]\n","[50,  6000] loss: [0.039, 0.042, 0.166, 0.143]\n","[50,  8000] loss: [0.037, 0.049, 0.186, 0.153]\n","[50, 10000] loss: [0.040, 0.040, 0.172, 0.141]\n","[50, 12000] loss: [0.108, 0.048, 0.159, 0.164]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-gBsrwmR0jmS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}